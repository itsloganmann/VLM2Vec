{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a9dc092",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsloganmann/VLM2Vec/blob/main/notebooks/run_mmeb_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b997e7",
      "metadata": {
        "id": "b5b997e7"
      },
      "source": [
        "# VLM2Vec MMEB-V2 Multi-Vector Benchmark\n",
        "\n",
        "This Colab notebook provisions an NVIDIA A100 (40 GB or 80 GB) workflow that installs pinned dependencies, clones the repo, materialises the Nemo retriever module, and evaluates nvidia/llama-nemoretriever-colembed-3b-v1 on MMEB-V2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68ad3ef",
      "metadata": {
        "id": "c68ad3ef"
      },
      "source": [
        "## Runtime checklist\n",
        "- Select **GPU** runtime with **NVIDIA A100 (40 GB or 80 GB)**.\n",
        "- Ensure Google Drive access for persistent caches, logs, metrics, and resume state.\n",
        "- Run cells sequentially; smoke mode validates setup before running the full benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808ac3d4",
      "metadata": {
        "id": "808ac3d4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "GPU_QUERY = [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"]\n",
        "\n",
        "\n",
        "def _parse_gpu_line(line: str) -> dict:\n",
        "    parts = [segment.strip() for segment in line.split(\",\")]\n",
        "    name = parts[0] if parts else line\n",
        "    memory_mib = None\n",
        "    if len(parts) > 1:\n",
        "        try:\n",
        "            memory_mib = int(parts[1].split()[0])\n",
        "        except (ValueError, IndexError):\n",
        "            memory_mib = None\n",
        "    memory_gib = round(memory_mib / 1024, 2) if memory_mib else None\n",
        "    tier = None\n",
        "    if \"A100\" in name and memory_mib:\n",
        "        if memory_mib >= 78000:\n",
        "            tier = \"80GB\"\n",
        "        elif memory_mib >= 39000:\n",
        "            tier = \"40GB\"\n",
        "        else:\n",
        "            tier = \"A100\"\n",
        "    return {\n",
        "        \"raw\": line,\n",
        "        \"name\": name,\n",
        "        \"memory_mib\": memory_mib,\n",
        "        \"memory_gib\": memory_gib,\n",
        "        \"tier\": tier,\n",
        "    }\n",
        "\n",
        "\n",
        "try:\n",
        "    raw_lines = subprocess.check_output(GPU_QUERY).decode(\"utf-8\").strip().splitlines()\n",
        "except Exception as exc:\n",
        "    raw_lines = []\n",
        "    print(f\"nvidia-smi unavailable: {exc}\")\n",
        "\n",
        "devices = [_parse_gpu_line(line) for line in raw_lines if line.strip()]\n",
        "\n",
        "print(\"Detected accelerators:\")\n",
        "if not devices:\n",
        "    print(\" - none\")\n",
        "else:\n",
        "    for dev in devices:\n",
        "        memory_txt = f\" ({dev['memory_gib']} GiB)\" if dev[\"memory_gib\"] else \"\"\n",
        "        print(f\" - {dev['name']}{memory_txt}\")\n",
        "\n",
        "primary = devices[0] if devices else None\n",
        "has_a100 = any(\"A100\" in dev[\"name\"] for dev in devices)\n",
        "a100_tiers = sorted({dev[\"tier\"] for dev in devices if dev[\"tier\"]})\n",
        "\n",
        "state = {\n",
        "    \"devices\": devices,\n",
        "    \"has_a100\": has_a100,\n",
        "    \"a100_tiers\": a100_tiers,\n",
        "    \"primary\": primary,\n",
        "}\n",
        "\n",
        "Path(\"/content/work\").mkdir(parents=True, exist_ok=True)\n",
        "with open(\"/content/work/gpu_detection.json\", \"w\") as fp:\n",
        "    json.dump(state, fp, indent=2)\n",
        "\n",
        "if not devices:\n",
        "    raise SystemExit(\"No NVIDIA GPU detected. Switch to an A100 runtime.\")\n",
        "\n",
        "if has_a100:\n",
        "    if a100_tiers:\n",
        "        tiers = \", \".join(a100_tiers)\n",
        "        print(f\"✅ NVIDIA A100 detected ({tiers}).\")\n",
        "    else:\n",
        "        print(\"✅ NVIDIA A100 detected.\")\n",
        "else:\n",
        "    suggested = {\n",
        "        \"device\": primary[\"name\"] if primary else \"unknown\",\n",
        "        \"adjustments\": {\n",
        "            \"max_batch_queries\": 2,\n",
        "            \"max_batch_docs\": 3,\n",
        "            \"patch_budget\": 768,\n",
        "        },\n",
        "    }\n",
        "    print(\"⚠️ Running without A100. Switch to quick smoke preset or adjust batch sizes.\")\n",
        "    print(json.dumps(suggested, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a0449e",
      "metadata": {
        "id": "b4a0449e"
      },
      "outputs": [],
      "source": [
        "%%capture install_log\n",
        "%pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ebc583",
      "metadata": {},
      "source": [
        "Quick check to ensure the Transformers wheel exposes the processors we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba23bc86",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "print('AutoProcessor import succeeded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53797b47",
      "metadata": {
        "id": "53797b47"
      },
      "outputs": [],
      "source": [
        "import logging, torch, sys\n",
        "from rich.console import Console\n",
        "\n",
        "console = Console(record=True)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(name)s | %(message)s')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "console.print(f'[bold green]Torch device[/bold green]: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else\n",
        "}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f87f5a1",
      "metadata": {
        "id": "1f87f5a1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE = Path('/content/work')\n",
        "PERSIST = Path('/content/drive/MyDrive/vlm2vec')\n",
        "for path in [BASE, PERSIST, PERSIST / 'outputs', PERSIST / 'cache', PERSIST / 'logs', PERSIST / 'profiler']:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "print('Workspace ready:', BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019982bb",
      "metadata": {
        "id": "019982bb"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd /content/work\n",
        "rm -rf VLM2Vec\n",
        "git clone https://github.com/itsloganmann/VLM2Vec.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7424c3d4",
      "metadata": {
        "id": "7424c3d4"
      },
      "source": [
        "## Materialise project assets\n",
        "This step copies the repository (including the NemoRetriever-only configs, evaluation harness, and tests) into `/content/work` so everything can run locally within the Colab runtime cache.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0722cb3b",
      "metadata": {
        "id": "0722cb3b"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "SRC = Path('/content/work/VLM2Vec')\n",
        "DST = Path('/content/work/runtime')\n",
        "if DST.exists():\n",
        "    shutil.rmtree(DST)\n",
        "shutil.copytree(SRC, DST)\n",
        "print('Copied project to', DST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4482b85",
      "metadata": {
        "id": "f4482b85"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run(['pytest'], cwd='/content/work/runtime', check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55379fa2",
      "metadata": {},
      "source": [
        "## Smoke evaluation (NemoRetriever)\n",
        "\n",
        "\n",
        "Run the lightweight NemoRetriever config first to validate the environment and caching before the longer full benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093cd2af",
      "metadata": {
        "id": "093cd2af"
      },
      "outputs": [],
      "source": [
        "!python /content/work/runtime/evaluation/multi_vector_eval.py --config /content/work/runtime/configs/mmeb_quick_smoke.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205700e6",
      "metadata": {
        "id": "205700e6"
      },
      "source": [
        "## Full evaluation\n",
        "Run the full MMEB-V2 benchmark with the NemoRetriever preset after the smoke test passes. Ensure the runtime stays connected (expect several hours on an A100).\n",
        "\n",
        "```python\n",
        "!python /content/work/runtime/evaluation/multi_vector_eval.py --config /content/work/runtime/configs/mmeb_a100_full.yaml\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
