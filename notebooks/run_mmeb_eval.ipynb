{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a9dc092",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsloganmann/VLM2Vec/blob/main/notebooks/run_mmeb_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b997e7",
      "metadata": {
        "id": "b5b997e7"
      },
      "source": [
        "# VLM2Vec MMEB-V2 Multi-Vector Benchmark\n",
        "\n",
        "This Colab notebook provisions an NVIDIA A100 (40 GB or 80 GB) workflow that installs pinned dependencies, clones the repo, materialises retriever modules, and evaluates vidore/colqwen2.5-v0.2, nvidia/llama-nemoretriever-colembed-3b-v1, and nomic-ai/colnomic-embed-multimodal-3b on MMEB-V2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68ad3ef",
      "metadata": {
        "id": "c68ad3ef"
      },
      "source": [
        "## Runtime checklist\n",
        "- Select **GPU** runtime with **NVIDIA A100 (40 GB or 80 GB)**.\n",
        "- Ensure Google Drive access for persistent caches, logs, metrics, and resume state.\n",
        "- Run cells sequentially; smoke mode validates setup before running the full benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808ac3d4",
      "metadata": {
        "id": "808ac3d4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "GPU_QUERY = [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"]\n",
        "\n",
        "\n",
        "def _parse_gpu_line(line: str) -> dict:\n",
        "    parts = [segment.strip() for segment in line.split(\",\")]\n",
        "    name = parts[0] if parts else line\n",
        "    memory_mib = None\n",
        "    if len(parts) > 1:\n",
        "        try:\n",
        "            memory_mib = int(parts[1].split()[0])\n",
        "        except (ValueError, IndexError):\n",
        "            memory_mib = None\n",
        "    memory_gib = round(memory_mib / 1024, 2) if memory_mib else None\n",
        "    tier = None\n",
        "    if \"A100\" in name and memory_mib:\n",
        "        if memory_mib >= 78000:\n",
        "            tier = \"80GB\"\n",
        "        elif memory_mib >= 39000:\n",
        "            tier = \"40GB\"\n",
        "        else:\n",
        "            tier = \"A100\"\n",
        "    return {\n",
        "        \"raw\": line,\n",
        "        \"name\": name,\n",
        "        \"memory_mib\": memory_mib,\n",
        "        \"memory_gib\": memory_gib,\n",
        "        \"tier\": tier,\n",
        "    }\n",
        "\n",
        "\n",
        "try:\n",
        "    raw_lines = subprocess.check_output(GPU_QUERY).decode(\"utf-8\").strip().splitlines()\n",
        "except Exception as exc:\n",
        "    raw_lines = []\n",
        "    print(f\"nvidia-smi unavailable: {exc}\")\n",
        "\n",
        "devices = [_parse_gpu_line(line) for line in raw_lines if line.strip()]\n",
        "\n",
        "print(\"Detected accelerators:\")\n",
        "if not devices:\n",
        "    print(\" - none\")\n",
        "else:\n",
        "    for dev in devices:\n",
        "        memory_txt = f\" ({dev['memory_gib']} GiB)\" if dev[\"memory_gib\"] else \"\"\n",
        "        print(f\" - {dev['name']}{memory_txt}\")\n",
        "\n",
        "primary = devices[0] if devices else None\n",
        "has_a100 = any(\"A100\" in dev[\"name\"] for dev in devices)\n",
        "a100_tiers = sorted({dev[\"tier\"] for dev in devices if dev[\"tier\"]})\n",
        "\n",
        "state = {\n",
        "    \"devices\": devices,\n",
        "    \"has_a100\": has_a100,\n",
        "    \"a100_tiers\": a100_tiers,\n",
        "    \"primary\": primary,\n",
        "}\n",
        "\n",
        "Path(\"/content/work\").mkdir(parents=True, exist_ok=True)\n",
        "with open(\"/content/work/gpu_detection.json\", \"w\") as fp:\n",
        "    json.dump(state, fp, indent=2)\n",
        "\n",
        "if not devices:\n",
        "    raise SystemExit(\"No NVIDIA GPU detected. Switch to an A100 runtime.\")\n",
        "\n",
        "if has_a100:\n",
        "    if a100_tiers:\n",
        "        tiers = \", \".join(a100_tiers)\n",
        "        print(f\"✅ NVIDIA A100 detected ({tiers}).\")\n",
        "    else:\n",
        "        print(\"✅ NVIDIA A100 detected.\")\n",
        "else:\n",
        "    suggested = {\n",
        "        \"device\": primary[\"name\"] if primary else \"unknown\",\n",
        "        \"adjustments\": {\n",
        "            \"max_batch_queries\": 2,\n",
        "            \"max_batch_docs\": 3,\n",
        "            \"patch_budget\": 768,\n",
        "        },\n",
        "    }\n",
        "    print(\"⚠️ Running without A100. Switch to quick smoke preset or adjust batch sizes.\")\n",
        "    print(json.dumps(suggested, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a0449e",
      "metadata": {
        "id": "b4a0449e"
      },
      "outputs": [],
      "source": [
        "%%capture install_log\n",
        "%pip install -U pip==24.2\n",
        "%pip install --force-reinstall --no-cache-dir 'numpy<2'\n",
        "%pip install torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install transformers==4.57.1 accelerate==0.34.2 pillow==10.3.0 tqdm==4.66.5 numpy==1.26.4 pyyaml==6.0.2 datasets==2.20.0 huggingface-hub==0.23.4 sentencepiece==0.2.0 safetensors==0.4.3 einops==0.8.0 timm==1.0.7 pandas==2.2.2 rich==13.7.1 keybert==0.7.0 umap-learn==0.5.5 hdbscan==0.8.33 bertopic==0.16.0 evaluate==0.4.1 pynvml==11.5.0 pytest==8.3.2 pytest-cov==4.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53797b47",
      "metadata": {
        "id": "53797b47"
      },
      "outputs": [],
      "source": [
        "import logging, torch, sys\n",
        "from rich.console import Console\n",
        "\n",
        "console = Console(record=True)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(name)s | %(message)s')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "console.print(f'[bold green]Torch device[/bold green]: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else\n",
        "}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f87f5a1",
      "metadata": {
        "id": "1f87f5a1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE = Path('/content/work')\n",
        "PERSIST = Path('/content/drive/MyDrive/vlm2vec')\n",
        "for path in [BASE, PERSIST, PERSIST / 'outputs', PERSIST / 'cache', PERSIST / 'logs', PERSIST / 'profiler']:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "print('Workspace ready:', BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019982bb",
      "metadata": {
        "id": "019982bb"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "cd /content/work\n",
        "rm -rf VLM2Vec\n",
        "git clone https://github.com/itsloganmann/VLM2Vec.git\n",
        "pip install -e VLM2Vec --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7424c3d4",
      "metadata": {
        "id": "7424c3d4"
      },
      "source": [
        "## Materialise project assets\n",
        "The following cells copy repository configs, retriever modules, evaluation harness, and tests into `/content/work` so they can be executed and cached within the Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0722cb3b",
      "metadata": {
        "id": "0722cb3b"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "SRC = Path('/content/work/VLM2Vec')\n",
        "DST = Path('/content/work/runtime')\n",
        "if DST.exists():\n",
        "    shutil.rmtree(DST)\n",
        "shutil.copytree(SRC, DST)\n",
        "print('Copied project to', DST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4482b85",
      "metadata": {
        "id": "f4482b85"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run(['pytest'], cwd='/content/work/runtime', check=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093cd2af",
      "metadata": {
        "id": "093cd2af"
      },
      "outputs": [],
      "source": [
        "!python /content/work/runtime/evaluation/multi_vector_eval.py --config /content/work/runtime/configs/mmeb_quick_smoke.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205700e6",
      "metadata": {
        "id": "205700e6"
      },
      "source": [
        "## Full evaluation\n",
        "Run the full MMEB-V2 benchmark after the smoke test passes. Ensure the runtime remains connected (expect several hours on A100).\n",
        "\n",
        "```python\n",
        "!python /content/work/runtime/evaluation/multi_vector_eval.py --config /content/work/runtime/configs/mmeb_a100_full.yaml\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
